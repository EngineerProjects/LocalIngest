{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Capitaux 01: AZ Capitaux Processor Testing\n",
                "\n",
                "**Purpose**: Test AZ capital extraction from IPF_AZ data\n",
                "\n",
                "**Tests**:\n",
                "1. Read IPF_AZ bronze data\n",
                "2. Apply business filters\n",
                "3. Extract 8 capital types (SMP, LCI, PE, RD, RC limits...)\n",
                "4. Calculate capital normalization (100% basis)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "project_root = Path.cwd().parent.parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from azfr_fsspec_utils import fspath\n",
                "import azfr_fsspec_abfs\n",
                "\n",
                "azfr_fsspec_abfs.use()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Capitaux_AZ_Testing\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"✓ Spark {spark.version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.loaders.config_loader import ConfigLoader\n",
                "from utils.loaders.transformation_loader import TransformationLoader\n",
                "import json\n",
                "\n",
                "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
                "loader = TransformationLoader(str(project_root / \"config\" / \"transformations\"))\n",
                "\n",
                "# Load capitaux extraction config\n",
                "cap_config_path = project_root / \"config\" / \"transformations\" / \"capitaux_extraction_config.json\"\n",
                "with open(cap_config_path, 'r') as f:\n",
                "    capital_config = json.load(f)\n",
                "\n",
                "# Remove comments\n",
                "capital_types = {k: v for k, v in capital_config.items() if not k.startswith('_')}\n",
                "\n",
                "print(f\"Capital types to extract: {list(capital_types.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Read Bronze Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.reader import BronzeReader\n",
                "\n",
                "VISION = \"202509\"\n",
                "\n",
                "bronze_reader = BronzeReader(\n",
                "    spark, config, \n",
                "    str(project_root / \"config\" / \"reading_config.json\")\n",
                ")\n",
                "\n",
                "df = bronze_reader.read_file_group('ipf_az', VISION)\n",
                "print(f\"✓ Read {df.count():,} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Apply Business Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CORRECTED: Fixed import path\n",
                "from utils.transformations import apply_business_filters\n",
                "\n",
                "business_rules = loader.get_business_rules()\n",
                "az_filters = business_rules['business_filters']['az']\n",
                "\n",
                "df_filtered = apply_business_filters(df, az_filters)\n",
                "print(f\"✓ After filters: {df_filtered.count():,} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Extract Capitals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.transformations import extract_capitals\n",
                "\n",
                "df_capitals = extract_capitals(df_filtered, capital_types)\n",
                "\n",
                "# Show extracted capitals\n",
                "capital_cols = [c for c in df_capitals.columns if any(\n",
                "    x in c for x in ['smp', 'lci', 'perte', 'risque', 'limite_rc']\n",
                ")]\n",
                "\n",
                "print(f\"✓ Capital columns created: {capital_cols}\")\n",
                "df_capitals.select('nopol', *capital_cols[:4]).show(5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Capital Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import sum as spark_sum, count, when, col\n",
                "\n",
                "for cap_col in capital_cols[:4]:\n",
                "    if cap_col in df_capitals.columns:\n",
                "        stats = df_capitals.agg(\n",
                "            count(when(col(cap_col) > 0, True)).alias('non_zero'),\n",
                "            spark_sum(cap_col).alias('total')\n",
                "        ).collect()[0]\n",
                "        print(f\"{cap_col}: {stats['non_zero']:,} non-zero, total={stats['total']:,.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"AZ CAPITAUX TESTING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Capital types: {list(capital_types.keys())}\")\n",
                "print(\"\\n→ Next: Notebook 02 - AZEC Capitaux\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}