{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitaux 01: AZ Capitaux Processor Testing\n",
    "\n",
    "**Purpose**: Test AZ capital extraction from IPF_AZ data\n",
    "\n",
    "**Tests**:\n",
    "1. Read IPF_AZ bronze data\n",
    "2. Apply business filters\n",
    "3. Extract 8 capital types (SMP, LCI, PE, RD, RC limits...)\n",
    "4. Calculate capital normalization (100% basis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspace/new_python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/17 20:14:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from azfr_fsspec_utils import fspath\n",
    "# import azfr_fsspec_abfs\n",
    "\n",
    "# azfr_fsspec_abfs.use()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capitaux_AZ_Testing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capital types to extract: ['smp_100', 'lci_100', 'perte_exp_100', 'risque_direct_100', 'limite_rc_100_par_sin', 'limite_rc_100_par_sin_tous_dom', 'limite_rc_100_par_an', 'smp_pe_100', 'smp_rd_100']\n"
     ]
    }
   ],
   "source": [
    "from utils.loaders.config_loader import ConfigLoader\n",
    "from utils.loaders.transformation_loader import TransformationLoader\n",
    "import json\n",
    "\n",
    "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
    "loader = TransformationLoader(str(project_root / \"config\" / \"transformations\"))\n",
    "\n",
    "# Load capitaux extraction config\n",
    "cap_config_path = project_root / \"config\" / \"transformations\" / \"capitaux_extraction_config.json\"\n",
    "with open(cap_config_path, 'r') as f:\n",
    "    capital_config = json.load(f)\n",
    "\n",
    "# Remove comments\n",
    "capital_types = {k: v for k, v in capital_config.items() if not k.startswith('_')}\n",
    "\n",
    "print(f\"Capital types to extract: {list(capital_types.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Bronze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Read 30,000 rows\n"
     ]
    }
   ],
   "source": [
    "from src.reader import BronzeReader\n",
    "\n",
    "VISION = \"202509\"\n",
    "\n",
    "bronze_reader = BronzeReader(\n",
    "    spark, config, \n",
    "    str(project_root / \"config\" / \"reading_config.json\")\n",
    ")\n",
    "\n",
    "df = bronze_reader.read_file_group('ipf_az', VISION)\n",
    "print(f\"✓ Read {df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Business Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ After filters: 30,000 rows\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED: Fixed import path\n",
    "from utils.transformations import apply_business_filters\n",
    "\n",
    "business_rules = loader.get_business_rules()\n",
    "az_filters = business_rules['business_filters']['az']\n",
    "\n",
    "df_filtered = apply_business_filters(df, az_filters)\n",
    "print(f\"✓ After filters: {df_filtered.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Capital columns created: ['mtsmpr', 'smp_100', 'lci_100', 'perte_exp_100', 'risque_direct_100', 'limite_rc_100_par_sin', 'limite_rc_100_par_sin_tous_dom', 'limite_rc_100_par_an', 'smp_pe_100', 'smp_rd_100']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/17 20:14:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+---------+-------------+\n",
      "|      nopol| mtsmpr|  smp_100|  lci_100|perte_exp_100|\n",
      "+-----------+-------+---------+---------+-------------+\n",
      "|POL00000001| 684.33|344605.86|192627.18|    197686.05|\n",
      "|POL00000002|1131.03|360173.35|      0.0|          0.0|\n",
      "|POL00000003| 481.65| 24260.07|248216.67|     16022.81|\n",
      "|POL00000004| 434.63|148891.11|197332.76|    494283.78|\n",
      "|POL00000005| 225.72|235841.25| 254365.7|          0.0|\n",
      "+-----------+-------+---------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.transformations import extract_capitals\n",
    "\n",
    "df_capitals = extract_capitals(df_filtered, capital_types)\n",
    "\n",
    "# Show extracted capitals\n",
    "capital_cols = [c for c in df_capitals.columns if any(\n",
    "    x in c for x in ['smp', 'lci', 'perte', 'risque', 'limite_rc']\n",
    ")]\n",
    "\n",
    "print(f\"✓ Capital columns created: {capital_cols}\")\n",
    "df_capitals.select('nopol', *capital_cols[:4]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Capital Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mtsmpr: 30,000 non-zero, total=29,861,042\n",
      "smp_100: 27,904 non-zero, total=7,088,350,434\n",
      "lci_100: 21,545 non-zero, total=5,469,309,310\n",
      "perte_exp_100: 20,386 non-zero, total=5,244,533,111\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, count, when, col\n",
    "\n",
    "for cap_col in capital_cols[:4]:\n",
    "    if cap_col in df_capitals.columns:\n",
    "        stats = df_capitals.agg(\n",
    "            count(when(col(cap_col) > 0, True)).alias('non_zero'),\n",
    "            spark_sum(cap_col).alias('total')\n",
    "        ).collect()[0]\n",
    "        print(f\"{cap_col}: {stats['non_zero']:,} non-zero, total={stats['total']:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AZ CAPITAUX TESTING COMPLETE\n",
      "============================================================\n",
      "Capital types: ['smp_100', 'lci_100', 'perte_exp_100', 'risque_direct_100', 'limite_rc_100_par_sin', 'limite_rc_100_par_sin_tous_dom', 'limite_rc_100_par_an', 'smp_pe_100', 'smp_rd_100']\n",
      "\n",
      "→ Next: Notebook 02 - AZEC Capitaux\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AZ CAPITAUX TESTING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Capital types: {list(capital_types.keys())}\")\n",
    "print(\"\\n→ Next: Notebook 02 - AZEC Capitaux\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
