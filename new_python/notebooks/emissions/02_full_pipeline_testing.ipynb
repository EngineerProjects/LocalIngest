{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emissions 02: Full Pipeline Testing\n",
    "\n",
    "**Purpose**: Test complete Emissions pipeline (Bronze → Transform → Gold)\n",
    "\n",
    "**Outputs**:\n",
    "- primes_emises_{vision}_pol_garp (by guarantee)\n",
    "- primes_emises_{vision}_pol (aggregated by policy)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspace/new_python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/17 22:23:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from azfr_fsspec_utils import fspath\n",
    "# import azfr_fsspec_abfs\n",
    "\n",
    "# azfr_fsspec_abfs.use()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Emissions_Pipeline_Testing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline for vision: 202509\n"
     ]
    }
   ],
   "source": [
    "from utils.loaders.config_loader import ConfigLoader\n",
    "from utils.logger import PipelineLogger\n",
    "# CORRECTED: Proper import path\n",
    "from src.processors.emissions_processors.emissions_processor import EmissionsProcessor\n",
    "\n",
    "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
    "logger = PipelineLogger(\"emissions_test\")\n",
    "\n",
    "VISION = \"202509\"\n",
    "print(f\"Testing pipeline for vision: {VISION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Emissions Processor (Manual Steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-17 22:23:29 - emissions_test - INFO - Emissions Processor initialized\n",
      "Step 1: Reading bronze data...\n",
      "2025-12-17 22:23:29 - emissions_test - INFO - Reading One BI emissions data for vision 202509\n",
      "2025-12-17 22:23:32 - emissions_test - INFO - ✓ SUCCESS: Read 20,000 records from One BI (bronze)\n",
      "✓ Read: 20,000 rows\n",
      "\n",
      "Step 2: Transforming data...\n",
      "2025-12-17 22:23:33 - emissions_test - INFO - Starting Emissions transformations\n",
      "2025-12-17 22:23:33 - emissions_test - INFO - STEP 1: Lowercasing all columns\n",
      "2025-12-17 22:23:33 - emissions_test - INFO - STEP 2: Applying business filters\n",
      "✗ Error: [Errno 2] No such file or directory: 'config/transformations/emissions_config.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_34995/1053400508.py\", line 14, in <module>\n",
      "    df_pol_garp, df_pol = emissions_processor.transform(df, VISION)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/new_python/src/processors/emissions_processors/emissions_processor.py\", line 120, in transform\n",
      "    with open(emissions_config_path, 'r') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'config/transformations/emissions_config.json'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emissions_processor = EmissionsProcessor(spark, config, logger)\n",
    "    \n",
    "    # CORRECTED: Use read() + transform() instead of run()\n",
    "    # run() writes directly to gold and returns None\n",
    "    \n",
    "    # Step 1: Read bronze data\n",
    "    print(\"Step 1: Reading bronze data...\")\n",
    "    df = emissions_processor.read(VISION)\n",
    "    print(f\"✓ Read: {df.count():,} rows\")\n",
    "    \n",
    "    # Step 2: Transform (returns tuple of 2 DataFrames)\n",
    "    print(\"\\nStep 2: Transforming data...\")\n",
    "    df_pol_garp, df_pol = emissions_processor.transform(df, VISION)\n",
    "    \n",
    "    print(f\"✓ POL_GARP (by guarantee): {df_pol_garp.count():,} rows\")\n",
    "    print(f\"✓ POL (aggregated): {df_pol.count():,} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df_pol_garp = df_pol = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Output Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_pol_garp is not None:\n",
    "    print(\"POL_GARP Schema:\")\n",
    "    print(f\"  Columns: {df_pol_garp.columns}\")\n",
    "    print(\"\\nExpected: vision, dircom, cdpole, nopol, cdprod, noint, cgarp, cmarch, cseg, cssseg, cd_cat_min, primes_x, primes_n, mtcom_x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_pol is not None:\n",
    "    print(\"POL Schema:\")\n",
    "    print(f\"  Columns: {df_pol.columns}\")\n",
    "    print(\"\\nExpected: vision, dircom, cdpole, nopol, cdprod, noint, cmarch, cseg, cssseg, primes_x, primes_n, mtcom_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_pol_garp is not None:\n",
    "    print(\"POL_GARP output (by guarantee):\")\n",
    "    # CORRECTED: Use actual column names (primes_x, primes_n, cgarp)\n",
    "    df_pol_garp.select('nopol', 'cdprod', 'cgarp', 'primes_x', 'primes_n').show(5)\n",
    "    \n",
    "    print(\"\\nPOL output (aggregated by policy):\")\n",
    "    # CORRECTED: Use actual column names\n",
    "    df_pol.select('nopol', 'cdprod', 'primes_x', 'primes_n').show(5)\n",
    "else:\n",
    "    print(\"⚠ No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate Aggregation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_pol_garp is not None and df_pol is not None:\n",
    "    # Verify POL is aggregation of POL_GARP\n",
    "    print(\"Validation: POL should be aggregation of POL_GARP\")\n",
    "    \n",
    "    # Pick one policy\n",
    "    sample_nopol = df_pol.select('nopol').first()['nopol']\n",
    "    \n",
    "    print(f\"\\nSample policy: {sample_nopol}\")\n",
    "    \n",
    "    # POL_GARP for this policy (multiple guarantees)\n",
    "    garp_data = df_pol_garp.filter(df_pol_garp.nopol == sample_nopol).select('cgarp', 'primes_x')\n",
    "    print(\"\\nPOL_GARP (by guarantee):\")\n",
    "    garp_data.show()\n",
    "    \n",
    "    # POL for this policy (aggregated)\n",
    "    pol_data = df_pol.filter(df_pol.nopol == sample_nopol).select('primes_x')\n",
    "    print(\"POL (aggregated):\")\n",
    "    pol_data.show()\n",
    "    \n",
    "    print(\"✓ Validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optional: Write to Gold (Manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to write outputs manually\n",
    "# if df_pol_garp is not None and df_pol is not None:\n",
    "#     emissions_processor.write((df_pol_garp, df_pol), VISION)\n",
    "#     print(\"✓ Data written to gold layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EMISSIONS PIPELINE TESTING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVision: {VISION}\")\n",
    "print(f\"POL_GARP: {'✓' if df_pol_garp is not None else '✗'}\")\n",
    "print(f\"POL:      {'✓' if df_pol is not None else '✗'}\")\n",
    "print(\"\\nKey learnings:\")\n",
    "print(\"  1. Use read() + transform() for testing (run() writes directly)\")\n",
    "print(\"  2. transform() returns tuple of (df_pol_garp, df_pol)\")\n",
    "print(\"  3. Column names: primes_x, primes_n, cgarp, mtcom_x\")\n",
    "print(\"\\n→ Run production: python main.py --vision 202509 --component emissions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
