{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Emissions 01: Bronze Data & Filters Testing\n",
                "\n",
                "**Purpose**: Test reading One BI premium data and applying business filters\n",
                "\n",
                "**Tests**:\n",
                "1. Read rf_fr1_prm_dtl_midcorp_m from bronze\n",
                "2. Apply exclusions (intermediaries, guarantees, categories)\n",
                "3. Verify filter impact\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "project_root = Path.cwd().parent.parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from azfr_fsspec_utils import fspath\n",
                "import azfr_fsspec_abfs\n",
                "\n",
                "azfr_fsspec_abfs.use()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Emissions_Testing\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"✓ Spark {spark.version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.loaders.config_loader import ConfigLoader\n",
                "from src.reader import BronzeReader\n",
                "import json\n",
                "\n",
                "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
                "bronze_reader = BronzeReader(\n",
                "    spark, config,\n",
                "    str(project_root / \"config\" / \"reading_config.json\")\n",
                ")\n",
                "\n",
                "# Load emissions exclusions\n",
                "with open(project_root / \"config\" / \"transformations\" / \"emissions_config.json\") as f:\n",
                "    emissions_config = json.load(f)\n",
                "\n",
                "print(\"Exclusions loaded:\")\n",
                "print(f\"  Intermediaries: {len(emissions_config['excluded_intermediaries'])}\")\n",
                "print(f\"  Guarantees: {emissions_config['excluded_guarantees']}\")\n",
                "print(f\"  Categories: {emissions_config['excluded_categories']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Read One BI Premium Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VISION = \"202509\"\n",
                "\n",
                "try:\n",
                "    df = bronze_reader.read_file_group('onebi_emissions', VISION)\n",
                "    print(f\"✓ Read {df.count():,} rows\")\n",
                "    print(f\"  Columns: {len(df.columns)}\")\n",
                "    df.select('nopol', 'cdprod', 'noint', 'cdgarp').show(5)\n",
                "except Exception as e:\n",
                "    print(f\"⚠ Error reading data: {e}\")\n",
                "    df = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Apply Exclusion Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col\n",
                "\n",
                "if df is not None:\n",
                "    count_before = df.count()\n",
                "    \n",
                "    # Filter 1: Excluded intermediaries\n",
                "    df_f1 = df.filter(~col('noint').isin(emissions_config['excluded_intermediaries']))\n",
                "    print(f\"After intermediary filter: {df_f1.count():,}\")\n",
                "    \n",
                "    # Filter 2: Excluded guarantees\n",
                "    df_f2 = df_f1.filter(~col('cdgarp').isin(emissions_config['excluded_guarantees']))\n",
                "    print(f\"After guarantee filter: {df_f2.count():,}\")\n",
                "    \n",
                "    # Filter 3: Excluded categories\n",
                "    df_f3 = df_f2.filter(~col('cdcateg').isin(emissions_config['excluded_categories']))\n",
                "    print(f\"After category filter: {df_f3.count():,}\")\n",
                "    \n",
                "    count_after = df_f3.count()\n",
                "    print(f\"\\nTotal: {count_before:,} → {count_after:,} ({(count_before-count_after):,} filtered)\")\n",
                "    \n",
                "    df_filtered = df_f3\n",
                "else:\n",
                "    print(\"⚠ No data to filter\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"EMISSIONS BRONZE TESTING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\n→ Next: Notebook 02 - Full Pipeline\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}