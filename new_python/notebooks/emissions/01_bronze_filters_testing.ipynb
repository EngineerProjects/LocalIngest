{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Emissions 01: Bronze Data & Filters Testing\n",
                "\n",
                "**Purpose**: Test reading One BI premium data and applying business filters\n",
                "\n",
                "**Tests**:\n",
                "1. Read rf_fr1_prm_dtl_midcorp_m from bronze\n",
                "2. Apply exclusions (intermediaries, guarantees, categories)\n",
                "3. Verify filter impact\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "project_root = Path.cwd().parent.parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from azfr_fsspec_utils import fspath\n",
                "import azfr_fsspec_abfs\n",
                "\n",
                "azfr_fsspec_abfs.use()\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Emissions_Testing\") \\\n",
                "    .getOrCreate()\n        "\n","print(f\"✓ Spark {spark.version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.loaders.config_loader import ConfigLoader\n",
                "from src.reader import BronzeReader\n",
                "import json\n",
                "\n",
                "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
                "bronze_reader = BronzeReader(\n",
                "    spark, config,\n",
                "    str(project_root / \"config\" / \"reading_config.json\")\n",
                ")\n",
                "\n",
                "# Load emissions exclusions\n",
                "with open(project_root / \"config\" / \"transformations\" / \"emissions_config.json\") as f:\n",
                "    emissions_config = json.load(f)\n",
                "\n",
                "print(\"Exclusions loaded:\")\n",
                "print(f\"  Intermediaries: {len(emissions_config['excluded_intermediaries'])}\")\n",
                "print(f\"  Guarantees: {emissions_config['excluded_guarantees']}\")\n",
                "print(f\"  Categories: {emissions_config['excluded_categories']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Read One BI Premium Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "VISION = \"202509\"\n",
                "\n",
                "try:\n",
                "    # CORRECTED: Use correct file_group name\n",
                "    df = bronze_reader.read_file_group('rf_fr1_prm_dtl_midcorp_m', VISION)\n",
                "    print(f\"✓ Read {df.count():,} rows\")\n",
                "    print(f\"  Columns: {len(df.columns)}\")\n",
                "    \n",
                "    # CORRECTED: Use BRONZE column names (raw, before transformation)\n",
                "    df.select('nu_cnt_prm', 'cd_prd_prm', 'cd_int_stc', 'cd_gar_prospctiv').show(5)\n",
                "except Exception as e:\n",
                "    print(f\"⚠ Error reading data: {e}\")\n",
                "    df = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Lowercase Columns (REQUIRED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ADDED: Lowercase before applying filters\n",
                "from utils.transformations import lowercase_all_columns\n",
                "\n",
                "if df is not None:\n",
                "    df = lowercase_all_columns(df)\n",
                "    print(\"✓ Columns lowercased\")\n",
                "    print(f\"  Sample columns: {df.columns[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Apply Exclusion Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col\n",
                "\n",
                "if df is not None:\n",
                "    count_before = df.count()\n",
                "    \n",
                "    # Filter 1: Market filter (cd_marche = '6')\n",
                "    df_f1 = df.filter(col('cd_marche') == '6')\n",
                "    print(f\"After market filter: {df_f1.count():,}\")\n",
                "    \n",
                "    # Filter 2: Date filter (dt_cpta_cts <= vision)\n",
                "    df_f2 = df_f1.filter(col('dt_cpta_cts') <= VISION)\n",
                "    print(f\"After date filter: {df_f2.count():,}\")\n",
                "    \n",
                "    # Filter 3: Excluded intermediaries (CORRECTED column name)\n",
                "    df_f3 = df_f2.filter(~col('cd_int_stc').isin(emissions_config['excluded_intermediaries']))\n",
                "    print(f\"After intermediary filter: {df_f3.count():,}\")\n",
                "    \n",
                "    # Filter 4: Excluded guarantees (CORRECTED column name)\n",
                "    df_f4 = df_f3.filter(~col('cd_gar_prospctiv').isin(emissions_config['excluded_guarantees']))\n",
                "    print(f\"After guarantee filter: {df_f4.count():,}\")\n",
                "    \n",
                "    # Filter 5: Excluded categories (CORRECTED column name)\n",
                "    df_f5 = df_f4.filter(~col('cd_cat_min').isin(emissions_config['excluded_categories']))\n",
                "    print(f\"After category filter: {df_f5.count():,}\")\n",
                "    \n",
                "    count_after = df_f5.count()\n",
                "    print(f\"\\nTotal: {count_before:,} → {count_after:,} ({(count_before-count_after):,} filtered)\")\n",
                "    \n",
                "    df_filtered = df_f5\n",
                "else:\n",
                "    print(\"⚠ No data to filter\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Verify Bronze Column Names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df_filtered is not None:\n",
                "    print(\"Bronze column mapping:\")\n",
                "    print(\"  nu_cnt_prm → nopol (after transformation)\")\n",
                "    print(\"  cd_prd_prm → cdprod (after transformation)\")\n",
                "    print(\"  cd_int_stc → noint (after transformation)\")\n",
                "    print(\"  cd_gar_prospctiv → used to extract cgarp (chars 3-5)\")\n",
                "    print(\"\\n✓ All filters applied successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"EMISSIONS BRONZE TESTING COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\n→ Next: Notebook 02 - Full Pipeline\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}