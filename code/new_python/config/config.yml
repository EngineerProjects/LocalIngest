# =============================================================================
# Configuration Pipeline Construction
# Architecture : Bronze → Silver → Gold
# =============================================================================

# Métadonnées du pipeline
pipeline:
  name: "Construction Data Pipeline"
  version: "1.0"
  description: "Pipeline mouvements de portefeuille"

# Format de vision (période de traitement)
vision:
  format: "YYYYMM"
  validation:
    min_year: 2000
    max_year: 2100

# Paramètres d'exécution
runtime:
  vision_: "202512"

# Configuration environnement (Dev/Prod)
environment:
  # Container ADLS (override via variable env DATALAKE_CONTAINER)
  container: "abfs://shared@azfrdatalab.dfs.core.windows.net"

# Azure Data Lake Storage - Chemins des données
datalake:
  # Racine stable (gérée par l'équipe data, change rarement)
  data_root: "/ABR/P4D/ADC/DATAMARTS/CONSTRUCTION"

  # Templates de chemins pour chaque couche
  paths:
    bronze_monthly: "{container}{data_root}/bronze/{year}/{month}"
    bronze_reference: "{container}{data_root}/bronze/ref"
    silver: "{container}{data_root}/silver/{year}/{month}"
    gold: "{container}{data_root}/gold/{year}/{month}"

# Configuration écriture (couches Silver et Gold)
output:
  clean: true
  format: "delta" # Delta Lake recommandé (ACID)
  compression: "snappy"
  mode: "overwrite"
  vacuum_hours: 336 # Rétention 14 jours (336h)

# Composants du pipeline
components:
  ptf_mvt:
    enabled: false
    description: "Mouvements de portefeuille (AZ + AZEC)"

  capitaux:
    enabled: false
    description: "Extraction capitaux (SMP/LCI)"

  emissions:
    enabled: true
    description: "Traitement émissions"

# Configuration Spark
spark:
  app_name: "Construction_Pipeline"
  config:
    # Exécution adaptative
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
    "spark.sql.files.maxPartitionBytes": "134217728" # 128MB

    # Mémoire (ajuster selon cluster)
    "spark.driver.memory": "4g"
    "spark.executor.memory": "8g"

    # Compatibilité Parquet
    "spark.sql.parquet.datetimeRebaseModeInWrite": "CORRECTED"

# Fichiers de configuration
config_files:
  reading_config: "config/reading_config.json"

# Logs
logging:
  level: "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  date_format: "%Y-%m-%d %H:%M:%S"
  local_dir: "logs"
  filename_template: "pipeline_{vision}.log"
