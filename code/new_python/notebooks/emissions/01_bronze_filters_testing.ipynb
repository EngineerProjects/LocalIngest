{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emissions 01: Bronze Data & Filters Testing\n",
    "\n",
    "**Purpose**: Test reading One BI premium data and applying business filters\n",
    "\n",
    "**Tests**:\n",
    "1. Read rf_fr1_prm_dtl_midcorp_m from bronze\n",
    "2. Apply exclusions (intermediaries, guarantees, categories)\n",
    "3. Verify filter impact\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspace/new_python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/17 22:15:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.4.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from azfr_fsspec_utils import fspath\n",
    "# import azfr_fsspec_abfs\n",
    "\n",
    "# azfr_fsspec_abfs.use()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Emissions_Testing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclusions loaded:\n",
      "  Intermediaries: 22\n",
      "  Guarantees: ['180', '183', '184', '185']\n",
      "  Categories: ['792', '793']\n"
     ]
    }
   ],
   "source": [
    "from utils.loaders.config_loader import ConfigLoader\n",
    "from src.reader import BronzeReader\n",
    "import json\n",
    "\n",
    "config = ConfigLoader(str(project_root / \"config\" / \"config.yml\"))\n",
    "bronze_reader = BronzeReader(\n",
    "    spark, config,\n",
    "    str(project_root / \"config\" / \"reading_config.json\")\n",
    ")\n",
    "\n",
    "# Load emissions exclusions\n",
    "with open(project_root / \"config\" / \"transformations\" / \"emissions_config.json\") as f:\n",
    "    emissions_config = json.load(f)\n",
    "\n",
    "print(\"Exclusions loaded:\")\n",
    "print(f\"  Intermediaries: {len(emissions_config['excluded_intermediaries'])}\")\n",
    "print(f\"  Guarantees: {emissions_config['excluded_guarantees']}\")\n",
    "print(f\"  Categories: {emissions_config['excluded_categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read One BI Premium Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Read 20,000 rows\n",
      "  Columns: 16\n",
      "+-----------+----------+----------+----------------+\n",
      "| nu_cnt_prm|cd_prd_prm|cd_int_stc|cd_gar_prospctiv|\n",
      "+-----------+----------+----------+----------------+\n",
      "|POL00010331|     01006|  INT00296|         XX240YY|\n",
      "|POL00001683|     01102|  INT00019|         XX300YY|\n",
      "|POL00006112|     01102|  INT00162|         XX220YY|\n",
      "|POL00011553|     01087|  INT00115|         XX310YY|\n",
      "|POL00013684|     01142|  INT00039|         XX240YY|\n",
      "+-----------+----------+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VISION = \"202509\"\n",
    "\n",
    "try:\n",
    "    # CORRECTED: Use correct file_group name\n",
    "    df = bronze_reader.read_file_group('rf_fr1_prm_dtl_midcorp_m', VISION)\n",
    "    print(f\"✓ Read {df.count():,} rows\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "    \n",
    "    # CORRECTED: Use BRONZE column names (raw, before transformation)\n",
    "    df.select('nu_cnt_prm', 'cd_prd_prm', 'cd_int_stc', 'cd_gar_prospctiv').show(5)\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error reading data: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lowercase Columns (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Columns lowercased\n",
      "  Sample columns: ['cd_niv_2_stc', 'cd_int_stc', 'nu_cnt_prm', 'cd_prd_prm', 'cd_statu_cts']\n"
     ]
    }
   ],
   "source": [
    "# ADDED: Lowercase before applying filters\n",
    "from utils.transformations import lowercase_all_columns\n",
    "\n",
    "if df is not None:\n",
    "    df = lowercase_all_columns(df)\n",
    "    print(\"✓ Columns lowercased\")\n",
    "    print(f\"  Sample columns: {df.columns[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Exclusion Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After market filter: 20,000\n",
      "After date filter: 20,000\n",
      "After intermediary filter: 20,000\n",
      "After guarantee filter: 20,000\n",
      "After category filter: 20,000\n",
      "\n",
      "Total: 20,000 → 20,000 (0 filtered)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "if df is not None:\n",
    "    count_before = df.count()\n",
    "    \n",
    "    # Filter 1: Market filter (cd_marche = '6')\n",
    "    df_f1 = df.filter(col('cd_marche') == '6')\n",
    "    print(f\"After market filter: {df_f1.count():,}\")\n",
    "    \n",
    "    # Filter 2: Date filter (dt_cpta_cts <= vision)\n",
    "    df_f2 = df_f1.filter(col('dt_cpta_cts') <= VISION)\n",
    "    print(f\"After date filter: {df_f2.count():,}\")\n",
    "    \n",
    "    # Filter 3: Excluded intermediaries (CORRECTED column name)\n",
    "    df_f3 = df_f2.filter(~col('cd_int_stc').isin(emissions_config['excluded_intermediaries']))\n",
    "    print(f\"After intermediary filter: {df_f3.count():,}\")\n",
    "    \n",
    "    # Filter 4: Excluded guarantees (CORRECTED column name)\n",
    "    df_f4 = df_f3.filter(~col('cd_gar_prospctiv').isin(emissions_config['excluded_guarantees']))\n",
    "    print(f\"After guarantee filter: {df_f4.count():,}\")\n",
    "    \n",
    "    # Filter 5: Excluded categories (CORRECTED column name)\n",
    "    df_f5 = df_f4.filter(~col('cd_cat_min').isin(emissions_config['excluded_categories']))\n",
    "    print(f\"After category filter: {df_f5.count():,}\")\n",
    "    \n",
    "    count_after = df_f5.count()\n",
    "    print(f\"\\nTotal: {count_before:,} → {count_after:,} ({(count_before-count_after):,} filtered)\")\n",
    "    \n",
    "    df_filtered = df_f5\n",
    "else:\n",
    "    print(\"⚠ No data to filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Bronze Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze column mapping:\n",
      "  nu_cnt_prm → nopol (after transformation)\n",
      "  cd_prd_prm → cdprod (after transformation)\n",
      "  cd_int_stc → noint (after transformation)\n",
      "  cd_gar_prospctiv → used to extract cgarp (chars 3-5)\n",
      "\n",
      "✓ All filters applied successfully\n"
     ]
    }
   ],
   "source": [
    "if df_filtered is not None:\n",
    "    print(\"Bronze column mapping:\")\n",
    "    print(\"  nu_cnt_prm → nopol (after transformation)\")\n",
    "    print(\"  cd_prd_prm → cdprod (after transformation)\")\n",
    "    print(\"  cd_int_stc → noint (after transformation)\")\n",
    "    print(\"  cd_gar_prospctiv → used to extract cgarp (chars 3-5)\")\n",
    "    print(\"\\n✓ All filters applied successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EMISSIONS BRONZE TESTING COMPLETE\n",
      "============================================================\n",
      "\n",
      "→ Next: Notebook 02 - Full Pipeline\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EMISSIONS BRONZE TESTING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n→ Next: Notebook 02 - Full Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
