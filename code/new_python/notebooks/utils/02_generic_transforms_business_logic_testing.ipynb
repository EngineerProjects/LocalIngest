{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 02 - Generic Transformations & Business Logic Testing\n",
                "\n",
                "Tests for **core transformation functions** that power the pipeline.\n",
                "\n",
                "## Modules Tested\n",
                "1. **`utils/transformations/base/generic_transforms.py`**\n",
                "   - `apply_conditional_transform()` - when/otherwise logic from config\n",
                "   - `apply_business_filters()` - Filter DataFrame using config rules\n",
                "   - `apply_transformations()` - Apply series of transformations\n",
                "\n",
                "2. **`utils/transformations/operations/business_logic.py`**\n",
                "   - `extract_capitals()` - Extract SMP/LCI from label/amount fields\n",
                "   - `calculate_movements()` - Calculate AFN, RES, RPT, RPC, NBPTF\n",
                "   - `calculate_exposures()` - Calculate expo_ytd and expo_gli\n",
                "\n",
                "3. **`utils/transformations/base/column_operations.py`**\n",
                "   - `lowercase_all_columns()` - Column standardization\n",
                "   - `apply_column_config()` - Apply passthrough/rename/computed columns\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "project_root = Path().absolute().parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "print(f\"Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, lit\n",
                "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
                "\n",
                "# Create Spark session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"UtilsTransformTesting\") \\\n",
                "    .master(\"local[1]\") \\\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(f\"✓ Spark {spark.version} session created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.transformations.base.generic_transforms import (\n",
                "    apply_conditional_transform,\n",
                "    apply_business_filters,\n",
                "    apply_transformations\n",
                ")\n",
                "from utils.transformations.base.column_operations import (\n",
                "    lowercase_all_columns,\n",
                "    apply_column_config\n",
                ")\n",
                "from utils.transformations.operations.business_logic import (\n",
                "    extract_capitals,\n",
                "    calculate_movements,\n",
                "    calculate_exposures\n",
                ")\n",
                "from utils.helpers import compute_date_ranges\n",
                "\n",
                "print(\"✓ Transformation functions imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Column Operations Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test lowercase_all_columns\n",
                "print(\"Testing lowercase_all_columns:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test DataFrame with mixed case columns\n",
                "df_mixed = spark.createDataFrame([\n",
                "    (\"A001\", \"Policy1\", 1000),\n",
                "    (\"A002\", \"Policy2\", 2000)\n",
                "], [\"NOPOL\", \"PolicyName\", \"Amount\"])\n",
                "\n",
                "print(\"Original columns:\", df_mixed.columns)\n",
                "\n",
                "# Apply lowercase\n",
                "df_lower = lowercase_all_columns(df_mixed)\n",
                "print(\"After lowercase:\", df_lower.columns)\n",
                "print(\"✓ All columns lowercase\" if all(c.islower() for c in df_lower.columns) else \"✗ Some columns not lowercase\")\n",
                "\n",
                "df_lower.show(3, truncate=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test apply_column_config\n",
                "print(\"\\nTesting apply_column_config:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test DataFrame\n",
                "df_test = spark.createDataFrame([\n",
                "    (\"P001\", 100, None, 50),\n",
                "    (\"P002\", 200, 10, 75)\n",
                "], [\"nopol\", \"mtprprto\", \"txcede\", \"prcdci\"])\n",
                "\n",
                "# Define config\n",
                "config = {\n",
                "    'passthrough': ['nopol', 'mtprprto'],\n",
                "    'rename': {'prcdci': 'prcie'},\n",
                "    'computed': {\n",
                "        'tx': {\n",
                "            'type': 'coalesce_default',\n",
                "            'source_col': 'txcede',\n",
                "            'default': 0\n",
                "        }\n",
                "    },\n",
                "    'init': {\n",
                "        'dircom': ('AZ', StringType())\n",
                "    }\n",
                "}\n",
                "\n",
                "df_configured = apply_column_config(df_test, config, '202509', 2025, 9)\n",
                "\n",
                "print(\"Columns after config:\")\n",
                "print(f\"  {df_configured.columns}\")\n",
                "print(\"\\nData:\")\n",
                "df_configured.show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Conditional Transform Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test apply_conditional_transform - coassurance logic\n",
                "print(\"Testing apply_conditional_transform - Coassurance classification:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test data\n",
                "df_coass = spark.createDataFrame([\n",
                "    (\"P001\", \"1\", \"3\"),  # APERITION\n",
                "    (\"P002\", \"1\", \"4\"),  # ACCEPTEE\n",
                "    (\"P003\", \"1\", \"8\"),  # INTERNATIONALE\n",
                "    (\"P004\", \"1\", \"9\"),  # AUTRES\n",
                "    (\"P005\", \"0\", None),  # SANS COASSURANCE\n",
                "], [\"nopol\", \"cdpolgp1\", \"cdcoas\"])\n",
                "\n",
                "# Define coassurance config\n",
                "coass_config = {\n",
                "    'conditions': [\n",
                "        {'col': 'cdpolgp1', 'op': '==', 'value': '1', 'and_col': 'cdcoas', 'and_in': ['3', '6'], 'result': 'APERITION'},\n",
                "        {'col': 'cdpolgp1', 'op': '==', 'value': '1', 'and_col': 'cdcoas', 'and_in': ['4', '5'], 'result': 'COASS. ACCEPTEE'},\n",
                "        {'col': 'cdpolgp1', 'op': '==', 'value': '1', 'and_col': 'cdcoas', 'and_in': ['8'], 'result': 'ACCEPTATION INTERNATIONALE'},\n",
                "        {'col': 'cdpolgp1', 'op': '==', 'value': '1', 'and_col': 'cdcoas', 'and_not_in': ['3', '4', '5', '6', '8'], 'result': 'AUTRES'}\n",
                "    ],\n",
                "    'default': 'SANS COASSURANCE'\n",
                "}\n",
                "\n",
                "df_result = apply_conditional_transform(df_coass, 'coass', coass_config)\n",
                "\n",
                "print(\"Coassurance classification results:\")\n",
                "df_result.select('nopol', 'cdpolgp1', 'cdcoas', 'coass').show(truncate=False)\n",
                "\n",
                "# Verify each case\n",
                "results = df_result.select('nopol', 'coass').collect()\n",
                "print(\"\\nVerifications:\")\n",
                "print(f\"  ✓ P001: APERITION\" if results[0]['coass'] == 'APERITION' else f\"  ✗ P001: {results[0]['coass']}\")\n",
                "print(f\"  ✓ P002: COASS. ACCEPTEE\" if results[1]['coass'] == 'COASS. ACCEPTEE' else f\"  ✗ P002: {results[1]['coass']}\")\n",
                "print(f\"  ✓ P003: ACCEPTATION INTERNATIONALE\" if results[2]['coass'] == 'ACCEPTATION INTERNATIONALE' else f\"  ✗ P003: {results[2]['coass']}\")\n",
                "print(f\"  ✓ P004: AUTRES\" if results[3]['coass'] == 'AUTRES' else f\"  ✗ P004: {results[3]['coass']}\")\n",
                "print(f\"  ✓ P005: SANS COASSURANCE\" if results[4]['coass'] == 'SANS COASSURANCE' else f\"  ✗ P005: {results[4]['coass']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Business Filters Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test apply_business_filters\n",
                "print(\"Testing apply_business_filters - Construction market filters:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test data\n",
                "df_filters = spark.createDataFrame([\n",
                "    (\"P001\", \"6\", \"2\", \"R\", \"01\", \"H90061\"),  # Should be EXCLUDED (bad intermed)\n",
                "    (\"P002\", \"6\", \"2\", \"R\", \"01\", \"123456\"),  # Should PASS\n",
                "    (\"P003\", \"5\", \"2\", \"R\", \"01\", \"123456\"),  # Should be EXCLUDED (wrong market)\n",
                "    (\"P004\", \"6\", \"1\", \"R\", \"01\", \"123456\"),  # Should be EXCLUDED (wrong segment)\n",
                "    (\"P005\", \"6\", \"2\", \"X\", \"01\", \"123456\"),  # Should be EXCLUDED (bad nature)\n",
                "    (\"P006\", \"6\", \"2\", \"R\", \"4\", \"123456\"),   # Should be EXCLUDED (bad status)\n",
                "], [\"nopol\", \"cmarch\", \"cseg\", \"cdnatp\", \"cdsitp\", \"noint\"])\n",
                "\n",
                "print(f\"Original row count: {df_filters.count()}\")\n",
                "\n",
                "# Define filter config\n",
                "filter_config = {\n",
                "    'description': 'Construction market filters',\n",
                "    'filters': [\n",
                "        {'type': 'equals', 'column': 'cmarch', 'value': '6', 'description': 'Construction market only'},\n",
                "        {'type': 'equals', 'column': 'cseg', 'value': '2', 'description': 'Segment 2 only'},\n",
                "        {'type': 'in', 'column': 'cdnatp', 'values': ['R', 'O', 'T', 'C'], 'description': 'Valid natures'},\n",
                "        {'type': 'not_in', 'column': 'cdsitp', 'values': ['4', '5'], 'description': 'Exclude status 4,5'},\n",
                "        {'type': 'not_in', 'column': 'noint', 'values': ['H90061'], 'description': 'Exclude bad intermed'}\n",
                "    ]\n",
                "}\n",
                "\n",
                "df_filtered = apply_business_filters(df_filters, filter_config)\n",
                "\n",
                "print(f\"Filtered row count: {df_filtered.count()}\")\n",
                "print(f\"\\nRemaining policies:\")\n",
                "df_filtered.select('nopol', 'cmarch', 'cseg', 'cdnatp', 'cdsitp', 'noint').show(truncate=False)\n",
                "\n",
                "# Verify only P002 should pass all filters\n",
                "remaining = [row['nopol'] for row in df_filtered.collect()]\n",
                "print(\"\\nVerification:\")\n",
                "print(f\"  ✓ Only P002 remains\" if remaining == ['P002'] else f\"  ✗ Wrong policies: {remaining}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Capital Extraction Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test extract_capitals\n",
                "print(\"Testing extract_capitals - SMP and LCI extraction:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test data with capital labels and amounts\n",
                "df_capitals = spark.createDataFrame([\n",
                "    (\"P001\", \"SMP GLOBAL DU CONTRAT\", 1000000.0, \"LCI GLOBAL DU CONTRAT\", 500000.0),\n",
                "    (\"P002\", \"Capital quelconque\", 0.0, \"LCI GLOBALE\", 750000.0),\n",
                "    (\"P003\", \"SMP RETENU\", 2000000.0, \"Autre capital\", 0.0),\n",
                "], [\"nopol\", \"lbcapi1\", \"mtcapi1\", \"lbcapi2\", \"mtcapi2\"])\n",
                "\n",
                "# Define capital extraction config\n",
                "capital_config = {\n",
                "    'smp_100': {\n",
                "        'keywords': ['SMP GLOBAL DU CONTRAT', 'SMP RETENU'],\n",
                "        'exclude_keywords': [],\n",
                "        'label_prefix': 'lbcapi',\n",
                "        'amount_prefix': 'mtcapi',\n",
                "        'num_indices': 2\n",
                "    },\n",
                "    'lci_100': {\n",
                "        'keywords': ['LCI GLOBAL DU CONTRAT', 'LCI GLOBALE'],\n",
                "        'exclude_keywords': [],\n",
                "        'label_prefix': 'lbcapi',\n",
                "        'amount_prefix': 'mtcapi',\n",
                "        'num_indices': 2\n",
                "    }\n",
                "}\n",
                "\n",
                "df_extracted = extract_capitals(df_capitals, capital_config)\n",
                "\n",
                "print(\"Extracted capitals:\")\n",
                "df_extracted.select('nopol', 'smp_100', 'lci_100').show(truncate=False)\n",
                "\n",
                "# Verify results\n",
                "results = df_extracted.select('nopol', 'smp_100', 'lci_100').collect()\n",
                "print(\"\\nVerifications:\")\n",
                "print(f\"  ✓ P001 SMP=1000000\" if results[0]['smp_100'] == 1000000.0 else f\"  ✗ P001 SMP={results[0]['smp_100']}\")\n",
                "print(f\"  ✓ P001 LCI=500000\" if results[0]['lci_100'] == 500000.0 else f\"  ✗ P001 LCI={results[0]['lci_100']}\")\n",
                "print(f\"  ✓ P002 SMP=0 (no match)\" if results[1]['smp_100'] == 0.0 else f\"  ✗ P002 SMP={results[1]['smp_100']}\")\n",
                "print(f\"  ✓ P002 LCI=750000\" if results[1]['lci_100'] == 750000.0 else f\"  ✗ P002 LCI={results[1]['lci_100']}\")\n",
                "print(f\"  ✓ P003 SMP=2000000\" if results[2]['smp_100'] == 2000000.0 else f\"  ✗ P003 SMP={results[2]['smp_100']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Movement Calculations Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test calculate_movements\n",
                "print(\"Testing calculate_movements - AFN, RES, NBPTF:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Get date ranges for 202509\n",
                "dates = compute_date_ranges(\"202509\")\n",
                "\n",
                "# Create test data\n",
                "df_movements = spark.createDataFrame([\n",
                "    (\"P001\", \"2025-01-15\", \"2025-01-15\", None, None, None, \"R\", \"1\", 1000.0),  # AFN\n",
                "    (\"P002\", \"2024-01-01\", \"2024-01-01\", \"2025-05-20\", None, None, \"R\", \"1\", 2000.0),  # RES\n",
                "    (\"P003\", \"2020-01-01\", \"2020-01-01\", None, None, None, \"R\", \"1\", 3000.0),  # NBPTF\n",
                "], [\"nopol\", \"dtcrepol\", \"dteffan\", \"dtresilp\", \"dttraan\", \"dttraar\", \"cdnatp\", \"cdsitp\", \"primeto\"])\n",
                "\n",
                "# Column mapping\n",
                "column_mapping = {\n",
                "    'creation_date': 'dtcrepol',\n",
                "    'effective_date': 'dteffan',\n",
                "    'termination_date': 'dtresilp',\n",
                "    'transfer_start': 'dttraan',\n",
                "    'transfer_end': 'dttraar',\n",
                "    'type_col1': None,\n",
                "    'type_col2': None,\n",
                "    'type_col3': None\n",
                "}\n",
                "\n",
                "# Also need cssseg column for portugal\n",
                "df_movements = df_movements.withColumn('cssseg', lit('1'))\n",
                "\n",
                "df_mvts = calculate_movements(df_movements, dates, 2025, column_mapping)\n",
                "\n",
                "print(\"Movement indicators:\")\n",
                "df_mvts.select('nopol', 'nbafn', 'nbres', 'nbptf', 'primeto').show(truncate=False)\n",
                "\n",
                "# Verify\n",
                "results = df_mvts.select('nopol', 'nbafn', 'nbres', 'nbptf').collect()\n",
                "print(\"\\nVerifications:\")\n",
                "print(f\"  ✓ P001 is AFN (nbafn=1)\" if results[0]['nbafn'] == 1 else f\"  ✗ P001 nbafn={results[0]['nbafn']}\")\n",
                "print(f\"  ✓ P002 is RES (nbres=1)\" if results[1]['nbres'] == 1 else f\"  ✗ P002 nbres={results[1]['nbres']}\")\n",
                "print(f\"  ✓ P003 is NBPTF (nbptf=1)\" if results[2]['nbptf'] == 1 else f\"  ✗ P003 nbptf={results[2]['nbptf']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Exposure Calculations Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test calculate_exposures\n",
                "print(\"Testing calculate_exposures - expo_ytd and expo_gli:\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "# Create test data: Contract active all year\n",
                "df_exposure = spark.createDataFrame([\n",
                "    (\"P001\", \"2025-01-01\", None, \"R\", \"1\"),  # Full year exposure\n",
                "    (\"P002\", \"2025-05-01\", None, \"R\", \"1\"),  # Partial year (from May)\n",
                "    (\"P003\", \"2024-01-01\", \"2025-03-31\", \"R\", \"3\"),  # Terminated in March\n",
                "], [\"nopol\", \"dtcrepol\", \"dtresilp\", \"cdnatp\", \"cdsitp\"])\n",
                "\n",
                "# Column mapping\n",
                "exposure_mapping = {\n",
                "    'creation_date': 'dtcrepol',\n",
                "    'termination_date': 'dtresilp'\n",
                "}\n",
                "\n",
                "df_expo = calculate_exposures(df_exposure, dates, 2025, exposure_mapping)\n",
                "\n",
                "print(\"Exposure calculations:\")\n",
                "df_expo.select('nopol', 'dtcrepol', 'dtresilp', 'expo_ytd', 'expo_gli').show(truncate=False)\n",
                "\n",
                "results = df_expo.select('nopol', 'expo_ytd', 'expo_gli').collect()\n",
                "print(\"\\nVerifications (approximate):\")\n",
                "print(f\"  P001 expo_ytd: {results[0]['expo_ytd']:.4f} (expect ~0.75 for 9 months)\")\n",
                "print(f\"  P001 expo_gli: {results[0]['expo_gli']:.4f} (expect ~1.0 for full month)\")\n",
                "print(f\"  P002 expo_ytd: {results[1]['expo_ytd']:.4f} (expect ~0.42 for 5 months)\")\n",
                "print(f\"  P003 expo_ytd: {results[2]['expo_ytd']:.4f} (expect ~0.25 for Q1 only)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook tested:\n",
                "- ✅ Column operations (lowercase, config application)\n",
                "- ✅ Conditional transforms (coassurance classification)\n",
                "- ✅ Business filters (construction market filtering)\n",
                "- ✅ Capital extraction (SMP/LCI from label fields)\n",
                "- ✅ Movement calculations (AFN, RES, NBPTF)\n",
                "- ✅ Exposure calculations (expo_ytd, expo_gli)\n",
                "\n",
                "All tests focus on **visual inspection** of results with clear expected outcomes."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}