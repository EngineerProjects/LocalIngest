# Guide de Comparaison: NOPOL Count Tracking - SAS vs PySpark

## ğŸ“‹ Tableau de RÃ©fÃ©rence

| Ã‰tape | PySpark Log | SAS Ligne | Description | Attendu |
|-------|-------------|-----------|-------------|---------|
| **0** | `0_AFTER_READ` | L150 | AprÃ¨s lecture PTF16+PTF36 avec filtres | Nombre initial de polices |
| **5** | `5_AFTER_IPFM99` | L183 | AprÃ¨s LEFT join IPFM99 (01099 only) | **= Ã©tape 0** (left join n'ajoute pas de lignes) |
| **6** | `6_AFTER_CAPITALS` | L231 | AprÃ¨s extraction capitaux (LBCAPI/MTCAPI) | **= Ã©tape 5** (UPDATE ne change pas row count) |
| **7** | `7_BEFORE_MOVEMENTS` | L248 | Avant calcul mouvements | **= Ã©tape 6** |
| **8** | `8_AFTER_MOVEMENTS` | L286 | AprÃ¨s calcul AFN/RES/NBPTF | **= Ã©tape 7** (UPDATE ne change pas row count) |
| **13** | `13_BEFORE_SEGMENT` | L492 | Avant join segmentation | **= Ã©tape 8** |
| **14** | `14_AFTER_SEGMENT` | L502 | AprÃ¨s LEFT join Segment + PT_GEST | **Peut augmenter** si duplicats dans refs! |
| **14b** | `14_BEFORE_DEDUP` | L502 | AprÃ¨s joins, avant dedup | Doit montrer duplicats |
| **15** | `15_AFTER_DEDUP` | L507 | AprÃ¨s `dropDuplicates(['nopol'])` | **Total = Distinct nopol** |

---

## ğŸ” Points de ContrÃ´le Critiques

### âœ… Point 1: AFTER_READ = AFTER_IPFM99
```
Si [5_AFTER_IPFM99] > [0_AFTER_READ]
â†’ âŒ PROBLÃˆME: IPFM99 a des duplicats sur (cdpole, cdprod, nopol, noint)
â†’ LEFT join crÃ©e un produit cartÃ©sien
```

### âœ… Point 2: Tous les UPDATE ne changent pas le count
```
Si [6_AFTER_CAPITALS] â‰  [5_AFTER_IPFM99]
â†’ âŒ PROBLÃˆME MAJEUR: extract_capitals modifie le nombre de lignes!

Si [8_AFTER_MOVEMENTS] â‰  [7_BEFORE_MOVEMENTS]
â†’ âŒ PROBLÃˆME MAJEUR: calculate_movements modifie le nombre de lignes!
```

### âš ï¸ Point 3: Segmentation Join (Zone critique!)
```
Si [14_AFTER_SEGMENT] > [13_BEFORE_SEGMENT]
â†’ âš ï¸  Join segmentation ajoute des lignes
â†’ VÃ©rifier: df_segment a-t-il des duplicats sur (cprod, reseau)?
â†’ VÃ©rifier: PT_GEST a-t-il des duplicats sur (ptgst)?

DiffÃ©rence acceptÃ©e = [14_AFTER_SEGMENT] - [13_BEFORE_SEGMENT]
```

### âœ… Point 4: DÃ©duplication finale
```
[15_AFTER_DEDUP] doit avoir:
  Total rows = Distinct nopol (les deux doivent Ãªtre identiques)
  Duplicates = 0

Si Duplicates > 0 aprÃ¨s dÃ©dup
â†’ âŒ PROBLÃˆME CRITIQUE: dropDuplicates(['nopol']) ne fonctionne pas!
```

---

## ğŸ“Š Format de Sortie Attendu

```
================================================================================
DIAGNOSTIC: NOPOL COUNT TRACKING (Step-by-step comparison with SAS)
================================================================================
[0_AFTER_READ       ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[5_AFTER_IPFM99     ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[6_AFTER_CAPITALS   ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[7_BEFORE_MOVEMENTS ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[8_AFTER_MOVEMENTS  ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[13_BEFORE_SEGMENT  ] Total: 71,890 | Distinct nopol: 68,742 | Duplicates: 3,148
[14_AFTER_SEGMENT   ] Total: 75,234 | Distinct nopol: 68,742 | Duplicates: 6,492  âš ï¸ SUSPECT!
[14_BEFORE_DEDUP    ] Total: 75,234 | Distinct nopol: 68,742 | Duplicates: 6,492
[15_AFTER_DEDUP     ] Total: 68,742 | Distinct nopol: 68,742 | Duplicates:     0  âœ… OK
================================================================================
END DIAGNOSTIC: Compare counts above with SAS at each step
================================================================================
```

### InterprÃ©tation de l'exemple ci-dessus:
- âœ… Ã‰tapes 0-13: Stable (pas de duplicats ajoutÃ©s)
- âš ï¸ Ã‰tape 14: +3,344 duplicats ajoutÃ©s aprÃ¨s join segmentation
  - â†’ Investiguer df_segment ou PT_GEST pour duplicats
- âœ… Ã‰tape 15: DÃ©dup fonctionne (68,742 polices finales)

**Comparaison SAS**: 
- Si SAS a 68,742 polices finales â†’ âœ… **ParitÃ© atteinte!**
- Si SAS a moins (ex: 65,594) â†’ âŒ PySpark a encore des polices en trop

---

## ğŸ¯ Plan d'Action Selon les RÃ©sultats

### ScÃ©nario A: DiffÃ©rence dÃ¨s AFTER_READ
```
[0_AFTER_READ] PySpark: 75,000 | SAS: 68,000
â†’ Cause: Filtres bronze ou fichiers sources diffÃ©rents
â†’ Action: VÃ©rifier reading_config.json et filtres business_rules.json
```

### ScÃ©nario B: DiffÃ©rence aprÃ¨s IPFM99
```
[5_AFTER_IPFM99] > [0_AFTER_READ]
â†’ Cause: IPFM99 a des duplicats sur les clÃ©s de join
â†’ Action: DÃ©dupliquer IPFM99 AVANT le join
```

### ScÃ©nario C: DiffÃ©rence aprÃ¨s Segmentation
```
[14_AFTER_SEGMENT] > [13_BEFORE_SEGMENT]
â†’ Cause: df_segment ou PT_GEST ont des duplicats
â†’ Action: Ajouter dropDuplicates sur les tables de rÃ©fÃ©rence AVANT join
```

### ScÃ©nario D: DÃ©dup finale ne fonctionne pas
```
[15_AFTER_DEDUP] Duplicates > 0
â†’ Cause: dropDuplicates(['nopol']) ne s'exÃ©cute pas correctement
â†’ Action: VÃ©rifier la syntaxe Spark et ordonner AVANT dropDuplicates
```

---

## ğŸ“ Prochaine Ã‰tape

1. **ExÃ©cuter le pipeline AZ** avec le logging activÃ©
2. **Copier les logs** dans ce fichier (section ci-dessous)
3. **Comparer** avec SAS ligne par ligne
4. **Identifier** Ã  quelle Ã©tape la divergence apparaÃ®t
5. **Appliquer** le plan d'action correspondant

---

## ğŸ“Š Logs PySpark (Ã€ remplir aprÃ¨s exÃ©cution)

```
[Coller ici les logs de l'exÃ©cution PySpark]
```

---

## ğŸ“Š RÃ©fÃ©rences SAS (Ã€ remplir)

```
[Coller ici les counts SAS Ã  chaque Ã©tape si disponibles]
```
