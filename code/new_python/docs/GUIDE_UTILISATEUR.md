# üìò Guide du D√©veloppeur & Tutoriel de Configuration

Ce guide est con√ßu pour vous apprendre √† **comprendre, configurer et modifier** le pipeline de donn√©es Construction sans avoir n√©cessairement besoin de modifier le code Python.

## üåü 1. Vue d'Ensemble du Pipeline

Le pipeline suit une architecture standard "M√©daillon" (Bronze ‚Üí Silver ‚Üí Gold) adapt√©e aux besoins de l'assurance construction.

### Architecture Simplifi√©e

```mermaid
graph LR
    Input[Fichiers CSV Bruts] --> Bronze
    Bronze --> Reader[Lecteur Intelligent]
    Reader --> Silver[Donn√©es Nettoy√©es]
    Silver --> Transformations[Calculs M√©tier]
    Transformations --> Gold[Donn√©es Finales]
    Config[Configuration JSON/YAML] -.-> Reader
    Config -.-> Transformations
```

1.  **Bronze (Entr√©e)** : Les fichiers bruts d√©pos√©s par les syst√®mes amont (IMS, OneBI).
2.  **Lecteur (Reader)** : Un composant intelligent qui lit, nettoie et standardise les donn√©es en se basant sur la configuration.
3.  **Silver (Interm√©diaire)** : Donn√©es propres, typ√©es (dates, nombres) et filtr√©es.
4.  **Gold (Sortie)** : Donn√©es enrichies avec les r√®gles m√©tier (Capital, Mouvements, etc.), pr√™tes pour l'analyse.

---

## üõ†Ô∏è 2. Guide des Transformations Rapides (JSON)

La plupart des r√®gles m√©tier sont d√©finies dans des fichiers JSON situ√©s dans `config/transformations/`. Vous pouvez modifier ces r√®gles **sans toucher au code Python**.

### Exemple : Modifier un Filtre M√©tier

Supposons que vous vouliez changer les codes nature accept√©s pour le portefeuille AZ.

1.  Ouvrez `config/transformations/az_transformations.json`.
2.  Cherchez la section `business_filters`.

```json
"business_filters": [
    {
        "filter_name": "nature_police",
        "column": "cdnatp",
        "operator": "in",
        "values": ["R", "O", "T", "C"]  <-- Modifiez cette liste
    }
]
```

3.  Ajoutez ou retirez un code (ex: ajoutez `"D"`).
4.  Sauvegardez. Le pipeline prendra en compte ce changement √† la prochaine ex√©cution.

### Exemple : Changer un Mot-cl√© de Recherche de Capital

Si un nouveau libell√© appara√Æt pour le calcul du SMP :

1.  Ouvrez `config/transformations/az_transformations.json`.
2.  Cherchez `capital_extraction`.

```json
"smp_global": {
    "keywords": [
        "SMP GLOBAL",
        "SMP RETENU",
        "SINISTRE MAXIMUM POSSIBLE",
        "NOUVEAU LIBELL√â ICI"  <-- Ajoutez votre mot-cl√©
    ],
    "priority": 1
}
```

---

## üì• 3. Gestion des Entr√©es (`reading_config.json`)

Ce fichier est le **cerveau** du lecteur de donn√©es. Il dit au pipeline *quoi* lire et *comment* le lire.

### Structure d'un Groupe de Fichiers

Chaque type de fichier est d√©fini par un bloc dans `file_groups` :

```json
"ipf": {
    "description": "Fichiers portefeuille",
    "file_patterns": ["ipf16.csv", "ipf36.csv"],
    "schema": "ipf",
    "read_options": {
        "sep": "|",
        "encoding": "LATIN9"
    },
    "dynamic_columns": [...]
}
```

### Comment Ajouter ou Retirer des Colonnes ?

Le pipeline utilise des d√©finitions de colonnes centralis√©es dans `config/column_definitions.py`.

1.  **Pour retirer une colonne** :
    *   Allez dans `config/column_definitions.py`.
    *   Trouvez la d√©finition du sch√©ma (ex: `IPF_SCHEMA = "..."`).
    *   Supprimez la colonne de la cha√Æne de caract√®res (format DDL SQL).
    *   Le lecteur ignorera d√©sormais cette colonne lors de la lecture.

2.  **Pour ajouter une colonne existante dans le CSV** :
    *   Allez dans `config/column_definitions.py`.
    *   Ajoutez le nom et le type dans la cha√Æne DDL : `nom_colonne TYPE,` (ex: `nouvelle_col STRING,`).
    *   **Attention** : Le nom doit correspondre EXACTEMENT (insensible √† la casse) au nom dans le fichier CSV.

**Exemple concret** :

Fichier : `config/column_definitions.py`
```python
IPF_SCHEMA = """
    cdpole STRING,
    nopol STRING,
    dtcrepol DATE,
    nouvelle_colonne STRING  -- Ajout√©e ici
"""
```

Le pipeline lira maintenant `nouvelle_colonne` depuis les fichiers IPF.
    *   Trouvez la d√©finition du sch√©ma (ex: `IPF_SCHEMA = "..."`).
    *   Supprimez la colonne de la cha√Æne de caract√®res (format DDL SQL).
    *   Le lecteur ignorera d√©sormais cette colonne lors de la lecture.

2.  **Pour ajouter une colonne existante dans le CSV** :
    *   Allez dans `config/column_definitions.py`.
    *   Ajoutez le nom et le type dans la cha√Æne DDL : `nom_colonne TYPE,` (ex: `nouvelle_col STRING,`).
    *   **Attention** : Le nom doit correspondre EXACTEMENT (insensible √† la casse) au nom dans le fichier CSV.

### ‚ú® Fonctionnalit√© Avanc√©e : Colonnes Dynamiques

C'est une fonctionnalit√© puissante qui permet d'ajouter des informations qui ne sont **pas dans le fichier**, mais qui d√©pendent du **nom du fichier**.

**Cas d'usage** : Les fichiers `ipf16.csv` sont pour le P√¥le 1 (Agent), et `ipf36.csv` pour le P√¥le 3 (Courtage), mais cette info n'est pas une colonne du fichier.

**Configuration (`reading_config.json`)** :

```json
"dynamic_columns": [
    {
        "pattern": "*16*",       // Si le nom du fichier contient "16"
        "columns": {
            "cdpole": "1"        // Alors ajouter colonne "cdpole" avec valeur "1"
        }
    },
    {
        "pattern": "*36*",       // Si le nom du fichier contient "36"
        "columns": {
            "cdpole": "3"        // Alors ajouter colonne "cdpole" avec valeur "3"
        }
    }
]
```

Le `Reader` applique ces r√®gles √† la vol√©e lors de la lecture. C'est transparent pour le reste du pipeline.

---

## ‚öôÔ∏è 4. Configuration Globale (`config.yml`)

Ce fichier g√®re l'infrastructure et l'ex√©cution du pipeline. Il est essentiel pour s√©parer l'environnement (Dev/Prod) du code.

### Pourquoi est-il n√©cessaire ?

Il permet de d√©ployer le m√™me code sur diff√©rents environnements (Local, Recette, Prod) sans changer une seule ligne de Python. Seul ce fichier change.

### Sections Cl√©s √† Conna√Ætre

1.  **`datalake`** : D√©finit o√π sont les donn√©es.
    ```yaml
    datalake:
      data_root: "/ABR/P4D/..."  # Racine du datalake
      paths:
         bronze_monthly: "..."   # Mod√®le de chemin
    ```
    *Si les chemins Azure changent, modifiez ici.*

2.  **`components`** : Active/D√©sactive des parties du pipeline.
    ```yaml
    components:
      ptf_mvt:
        enabled: true   # Mettre false pour sauter cette √©tape
    ```
    *Utile pour ex√©cution par d√©faut.*

3.  **`spark`** : Optimisation des performances.
    ```yaml
    spark:
      config:
        "spark.driver.memory": "4g"  # Augmentez si erreur "Out of Memory"
    ```

---

## üöÄ 5. Tutoriel : Utiliser `main.py` Efficacement

Le script `main.py` est le point d'entr√©e unique. Il est con√ßu pour √™tre simple mais flexible.

### Commande de Base (Ex√©cution Standard)

Pour lancer le traitement normal pour une vision (mois) donn√©e :

```bash
python main.py --vision 202512
```

**Ce que √ßa fait** :
1.  Charge la configuration depuis `config/config.yml` (par d√©faut).
2.  Regarde la section `components` dans le fichier YAML.
3.  Ex√©cute **tous** les composants marqu√©s `enabled: true`.

### Ex√©cuter un Seul Composant (Force)

Si vous voulez uniquement recalculer les capitaux sans refaire tout le reste (m√™me si d√©sactiv√© dans la config) :

```bash
python main.py --component capitaux --vision 202512
```

L'option `--component` **ignore** l'√©tat `enabled` dans `config.yml`.
**Composants disponibles** : `ptf_mvt`, `capitaux`, `emissions`.

### Changer de Fichier de Configuration

Si vous voulez tester une configuration diff√©rente (ex: en local ou recette) :

```bash
python main.py --vision 202512 --config config/config_dev.yml
```

Cela permet d'avoir plusieurs fichiers `config.yml` pour diff√©rents environnements sans modifier le code.

### Mode Debug / Test

Pour tester sur un environnement local ou voir les logs en d√©tail :

1.  Modifiez `config.yml` pour mettre `logging.level: "DEBUG"`.
2.  Lancez avec une vision de test.

### Encha√Ænement Intelligent et Priorit√©s

1.  **Arguments CLI** (`--vision`) > **Variables d'env** (`PIPELINE_VISION`) > **D√©faut Config** (`runtime.vision_`).
2.  Le `main.py` initialise **une seule session Spark** partag√©e pour tout le pipeline (gain de temps).
3.  Si un composant √©choue, le pipeline s'arr√™te proprement et loggue l'erreur, mais ne bloque pas les composants pr√©c√©dents r√©ussis.

---

## üéì R√©sum√© pour le D√©veloppeur

| Je veux...                              | Fichier √† modifier                                |
| --------------------------------------- | ------------------------------------------------- |
| Changer un chemin de fichier            | `config/config.yml`                               |
| Changer le format d'un fichier d'entr√©e | `config/reading_config.json`                      |
| Ajouter une colonne lue                 | `config/column_definitions.py`                    |
| Changer une r√®gle de calcul m√©tier      | `config/transformations/*.json`                   |
| Changer la m√©moire Spark                | `config/config.yml`                               |
| Ajouter un nouveau flux                 | Cr√©er un nouveau processor dans `src/processors/` |

---

*Ce document doit vivre avec le projet. Mettez-le √† jour si vous ajoutez de nouvelles fonctionnalit√©s de configuration.*
