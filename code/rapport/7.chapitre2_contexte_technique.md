# CHAPITRE 2 - CONTEXTE TECHNIQUE ET ÉTAT DE L'ART

## 2.1 L'existant : le datamart Construction en SAS

### Rôle du datamart Construction dans l'analyse assurantielle

Le datamart Construction constitue l'un des cinq datamarts P&C gérés par l'équipe Data Management d'Allianz France. Il agrège et structure l'ensemble des données relatives au marché de l'assurance construction, qui couvre les risques liés aux bâtiments, travaux et chantiers. Ce marché présente des spécificités métier importantes : les capitaux assurés sont généralement élevés, les sinistres peuvent être complexes et s'étaler sur plusieurs années, et l'indexation des capitaux en fonction de l'inflation dans le secteur de la construction est cruciale.

Le datamart Construction alimente trois types d'analyses principales. Les **études de portefeuille** permettent de suivre l'évolution du nombre de polices, les mouvements d'affaires nouvelles et de résiliations, et la composition du portefeuille par segment de clientèle (agents, courtiers, entreprises de construction). Les **analyses de capitaux** portent sur l'évaluation de l'exposition au risque à travers les capitaux assurés (Sinistre Maximum Possible, Limite Contractuelle d'Indemnité, Perte d'Exploitation, Risque Direct) et leur indexation selon les indices de coût de construction de la Fédération Française du Bâtiment. Enfin, les **calculs de primes** agrègent les émissions de primes par police et par garantie pour alimenter les reporting financiers et réglementaires.

Les utilisateurs principaux de ce datamart sont les actuaires de la direction P4D, les équipes de pricing qui calibrent les tarifs commerciaux, les contrôleurs de gestion qui suivent la rentabilité du marché, et les équipes de pilotage qui produisent les reporting pour la direction générale et les autorités de régulation. La disponibilité et la fiabilité de ce datamart sont donc critiques pour le fonctionnement quotidien de l'entreprise.

### Architecture actuelle en SAS

L'implémentation actuelle du datamart Construction repose sur une architecture SAS développée progressivement sur plusieurs années. Le code est réparti sur **dix-neuf fichiers SAS**. Ces fichiers sont organisés selon une logique séquentielle : chaque fichier réalise une étape de transformation spécifique et produit des tables intermédiaires qui servent d'entrée au fichier suivant.

L'architecture SAS suit un modèle monolithique où toutes les transformations sont enchaînées dans un flux linéaire. Les données brutes, stockées sous forme de fichiers CSV sur des serveurs partagés, sont lues par les premiers fichiers SAS qui appliquent des filtres métier de base (marché Construction, exclusion des polices annulées). Les fichiers suivants enrichissent progressivement ces données : extraction des capitaux depuis des champs texte, calculs des mouvements de portefeuille, jointures avec des tables de référence (nomenclatures NAF, codes ISIC, données IRD), calculs d'indexation, et enfin agrégation pour produire les tables finales.

Cette architecture présente plusieurs caractéristiques techniques. Le **code SAS utilise massivement les macros**, mécanisme de programmation qui permet de générer dynamiquement du code SAS. Certaines macros sont imbriquées sur plusieurs niveaux, rendant la lecture du code complexe et nécessitant de dérouler mentalement l'ensemble des substitutions pour comprendre la logique réelle. Les **variables globales** sont utilisées pour partager des paramètres entre différents fichiers (chemins de données, période de traitement, codes métier), mais ces variables ne sont pas toujours documentées et leur portée n'est pas toujours claire. Les **transformations métier** sont mélangées avec la logique de lecture/écriture des fichiers, rendant difficile l'identification des règles de gestion pures. Enfin, **l'absence de documentation initiale** oblige à une lecture exhaustive du code pour comprendre l'ensemble des traitements.

Le processus d'exécution est séquentiel : les dix-neuf fichiers doivent être lancés dans un ordre précis, chaque fichier attendant que le précédent ait terminé avant de démarrer. Cette séquentialité limite les possibilités d'optimisation et allonge les temps de traitement globaux. Les données intermédiaires sont matérialisées sur disque entre chaque étape, ce qui génère des opérations d'entrée/sortie importantes et consomme de l'espace disque.

**[FIGURE 2.1 : Schéma de l'architecture SAS monolithique (flux séquentiel)]**

### Limites de l'architecture SAS existante

Au-delà des limites stratégiques évoquées dans le chapitre 1 (coûts de licences, raréfaction de l'expertise, performances limitées), l'architecture SAS présente plusieurs contraintes techniques qui ont motivé la décision de migration.

**La scalabilité est structurellement limitée.** SAS exécute les traitements sur un seul serveur de calcul. Lorsque les volumes de données augmentent ou que de nouveaux enrichissements sont ajoutés, les temps de traitement s'allongent proportionnellement. Il n'existe pas de mécanisme simple pour paralléliser les calculs ou distribuer la charge sur plusieurs machines. Cette contrainte devient problématique lorsque l'équipe doit traiter plusieurs années d'historique simultanément ou réaliser des recalculs massifs.

**La maintenabilité du code est coûteuse.** La complexité du code SAS, avec ses macros imbriquées et ses variables globales, rend les évolutions risquées. Lorsqu'une règle métier doit être modifiée (par exemple, ajout d'un nouveau type de capital ou évolution des critères de segmentation), il faut tracer l'ensemble des endroits où cette règle intervient, en tenant compte des substitutions de macros. Le risque d'effets de bord est élevé, et chaque modification nécessite des tests exhaustifs sur plusieurs visions pour s'assurer qu'aucune régression n'a été introduite.

**L'absence de séparation des responsabilités** complique le débogage et les tests. Dans l'architecture SAS, un même fichier peut mélanger la lecture des données, les filtres métier, les transformations, les jointures et l'écriture des résultats. Lorsqu'un problème survient (incohérence dans les résultats, erreur d'exécution), il est difficile d'isoler la cause : le problème vient-il de la lecture des données brutes, d'un filtre mal appliqué, d'une transformation incorrecte, ou d'une jointure mal spécifiée ?

**La gestion des versions et de la traçabilité est rudimentaire.** Le code SAS est stocké sur des répertoires partagés, sans système de gestion de versions moderne (pas de Git). Les commentaires dans le code sont rares et souvent obsolètes. Lorsqu'une règle métier a été modifiée, il est difficile de retrouver l'historique des changements, les raisons de ces changements, et les impacts sur les résultats produits.

Enfin, **l'environnement de développement et de test est limité.** Les modifications du code SAS doivent être testées directement sur les serveurs de production ou sur des environnements de pré-production qui ne sont pas toujours disponibles. Il n'existe pas de mécanisme simple pour tester localement une modification ou pour valider une règle de gestion sur un sous-ensemble de données avant de la déployer en production.

Ces limites techniques, combinées aux enjeux stratégiques de coûts et d'expertise, ont conduit à la décision de migrer vers une architecture moderne basée sur PySpark et Azure.

---

## 2.2 Technologies modernes pour le traitement de données

### PySpark et Apache Spark

**Apache Spark** est un framework open source de traitement distribué de données massives. Contrairement aux systèmes traditionnels qui traitent les données séquentiellement sur un seul serveur, Spark distribue automatiquement les calculs sur un cluster de machines, permettant de paralléliser les opérations. **PySpark** est l'interface Python de Spark, offrant une API intuitive pour manipuler des DataFrames (structures tabulaires) avec des performances optimisées pour le calcul distribué.

Spark présente plusieurs avantages pour le traitement de datamarts : traitement en mémoire pour de meilleures performances, support natif de multiples formats de fichiers (CSV, Parquet, Delta), écosystème riche de bibliothèques, et scalabilité horizontale (ajout de machines pour augmenter la puissance de calcul).

### Azure Databricks

**Azure Databricks** est une plateforme cloud managée qui fournit un environnement complet pour exécuter des workloads Spark. Créée en partenariat entre Microsoft et les créateurs d'Apache Spark, cette plateforme offre des clusters Spark gérés automatiquement, des notebooks collaboratifs pour le développement, une intégration native avec Azure Data Lake Storage (ADLS), et le support de Delta Lake pour la gestion avancée de données (transactions ACID, versioning).

Le choix d'Azure Databricks s'aligne avec la stratégie du groupe Allianz, qui a standardisé ses infrastructures data sur Azure au niveau international.

### Formats de stockage modernes

**Parquet** est un format de fichier binaire optimisé pour le stockage en colonnes. Il offre une compression efficace et des performances de lecture supérieures au CSV traditionnel, particulièrement pour les requêtes portant sur un sous-ensemble de colonnes.

**Delta Lake**, extension de Parquet, ajoute des fonctionnalités de gestion de données avancées : transactions ACID (garanties de cohérence), time travel (accès aux versions historiques), et gestion de schéma avec évolution contrôlée. Ces fonctionnalités améliorent la qualité et la traçabilité des données.

### Comparaison avec l'existant SAS

Le tableau ci-dessous synthétise les différences principales entre les deux stack technologiques :

| **Critère**       | **SAS (Ancien)**             | **PySpark (Nouveau)**        |
| ----------------- | ---------------------------- | ---------------------------- |
| **Architecture**  | Monolithique (flux linéaire) | Distribuée (parallélisation) |
| **Licences**      | Propriétaires (coûteuses)    | Open Source (gratuit)        |
| **Scalabilité**   | Limitée (serveur unique)     | Élevée (cluster élastique)   |
| **Configuration** | Hardcodée dans les scripts   | Externalisée (JSON/YAML)     |
| **Performance**   | Séquentielle                 | Parallèle (calcul distribué) |

**[FIGURE 2.3 : Tableau comparatif SAS vs PySpark]**

Cette évolution technologique permet de moderniser l'infrastructure tout en réduisant les coûts opérationnels et en améliorant les performances.
