# CHAPITRE 3 - MÉTHODOLOGIE

Le projet de migration du datamart Construction a été structuré selon une approche méthodologique en **cinq phases successives**, privilégiant l'analyse exhaustive avant le développement. Cette démarche reflète le principe directeur du projet : "Analyser et documenter exhaustivement avant d'écrire la première ligne de code".

**[FIGURE 3.1 : Schéma des 5 phases méthodologiques avec durées]**

---

## 3.1 Phase 1 : Comprendre l'existant (3 semaines)

### Objectifs

La première phase visait à acquérir une compréhension complète du datamart SAS existant. L'objectif était double : comprendre la logique métier encodée dans les 19 fichiers SAS, et identifier toutes les sources de données utilisées ainsi que les tables produites.

### Approche

L'analyse a consisté en une lecture exhaustive et méthodique du code SAS, fichier par fichier. Pour chaque fichier, les flux de données (quelles tables sont lues, quelles transformations sont appliquées, quelles tables sont produites) ont été tracés et les règles métier identifiées ont été documentées. Cette documentation était essentielle car le code SAS original ne contenait pratiquement aucun commentaire explicatif.

Les macros SAS, particulièrement complexes à comprendre, ont nécessité un travail de "déroulage mental" pour identifier la logique réelle après substitution des variables. Un mapping complet entre les noms de variables SAS et leur signification métier a été créé, validé régulièrement avec l'équipe.

### Livrables

Cette phase a produit une documentation détaillée de l'architecture SAS : inventaire complet des 19 fichiers avec leur rôle, cartographie des flux de données entre fichiers, recensement préliminaire des règles métier, et identification des dépendances externes (tables de référence, fichiers d'indices FFB). Cette documentation a servi de fondation pour toutes les phases suivantes.

---

## 3.2 Phase 2 : Recenser les règles métier et les datasets (1 semaine)

### Objectifs

Fort de la compréhension globale acquise en phase 1, cette deuxième phase visait à produire un inventaire exhaustif et structuré de tous les éléments du datamart : règles de gestion, sources de données, et tables produites.

### Approche

Un catalogue complet des **quarante-cinq datasets** (groupes de fichiers) utilisés par le datamart Construction a été créé, documentant pour chacun : le nom du dataset, son rôle (source Bronze, table de référence, table intermédiaire, output final), son format (CSV, SAS7BDAT), et sa fréquence de mise à jour. Ce catalogue a permis de garantir qu'aucune source de données ne serait oubliée lors de la migration.

Parallèlement, toutes les **règles métier** identifiées dans le code SAS ont été recensées : filtres sur le marché Construction (CMARCH=6, CSEG=2), règles d'extraction des capitaux (mots-clés à chercher dans les 14 colonnes LBCAPI), logique de calcul des mouvements de portefeuille (AFN, RES, PTF), règles de migration entre systèmes IMS et AZEC, et logique d'indexation FFB.

### Livrables

Le principal livrable de cette phase est le **Data Catalog**, document structuré inventoriant les 45 datasets avec leurs caractéristiques techniques et métier. Ce catalogue a servi de référence tout au long du projet pour s'assurer que l'implémentation Python couvrait bien l'ensemble du périmètre fonctionnel.

---

## 3.3 Phase 3 : Concevoir l'architecture cible (1 semaine)

### Objectifs

Cette phase de conception visait à définir l'architecture du nouveau datamart PySpark. L'enjeu était de créer une architecture moderne et maintenable tout en garantissant la capacité à reproduire exactement les résultats SAS.

### Choix de l'architecture médaillon

L'**architecture médaillon** (Bronze → Silver → Gold), standard de l'industrie pour les pipelines de données modernes, a été retenue. Cette architecture structure les transformations en trois couches distinctes :

**Bronze** : ingestion des données brutes au format Parquet, sans transformation métier. Cette couche constitue une copie fidèle des sources CSV, permettant de rejouer les transformations ultérieures si nécessaire.

**Silver** : application des transformations métier spécifiques à chaque canal (AZ et AZEC). Les règles de filtrage, les calculs de mouvements, les extractions de capitaux et les premiers enrichissements sont implémentés dans cette couche. Les données sont nettoyées, typées et structurées selon le modèle métier.

**Gold** : consolidation finale fusionnant AZ et AZEC, déduplication des polices présentes dans les deux canaux, et enrichissements finaux (ISIC, données clients, IRD). Cette couche produit les tables finales prêtes pour consommation par les utilisateurs métier.

**[FIGURE 3.2 : Architecture médaillon détaillée Bronze → Silver → Gold pour le datamart Construction]**

### Justification de ce choix

Cette architecture offre plusieurs bénéfices critiques pour le projet :

**Traçabilité** : chaque erreur peut être tracée jusqu'à sa source (problème d'ingestion Bronze ? transformation Silver incorrecte ? enrichissement Gold manquant ?)

**Rejouabilité** : à partir des données Bronze, on peut recalculer les couches Silver et Gold autant de fois que nécessaire sans relire les sources CSV originales

**Maintenabilité** : les évolutions métier futures se feront principalement dans Silver et Gold, sans toucher à l'ingestion Bronze

**Performance** : l'usage de Parquet et Delta Lake améliore drastiquement les temps de lecture comparé au CSV

### Organisation du code

L'architecture du code Python a été conçue pour être modulaire et réutilisable :

```
src/
    ├── processors/        # Logique métier par pipeline
    │   ├── ptf_mvt_processors/
    │   ├── capitaux_processor.py
    │   └── emissions_processor.py
    ├── readers/           # Lecture des données Bronze
    ├── helpers/           # Fonctions utilitaires réutilisables
    └── main.py           # Orchestrateur principal
config/
    ├── config.yml        # Configuration globale
    ├── reading_config.json   # Configuration de lecture Bronze
    └── transformations/  # Règles métier externalisées (JSON)
```

Cette structure sépare clairement les responsabilités : l'orchestration (`main.py`), la logique métier (`processors`), les helpers techniques (`readers`, `helpers`), et la configuration externalisée (`config/`).

---

## 3.4 Phase 4 : Développer les pipelines Python (5 semaines)

### Approche de développement

Le développement a suivi une approche itérative par pipeline, en commençant par le plus critique (PTF Mouvements) avant de traiter les autres (Capitaux, Émissions). Pour chaque pipeline, le cycle suivant a été appliqué : implémentation → tests unitaires → validation sur vision de référence → correction → tests sur visions multiples.

### Principes de développement retenus

Plusieurs principes ont guidé le développement pour garantir la qualité et la maintenabilité du code :

**Configuration externalisée** : toutes les règles métier (mots-clés pour extraction de capitaux, codes de filtrage, règles de segmentation) sont stockées dans des fichiers JSON/YAML, permettant de les modifier sans toucher au code Python.

**Modularité** : chaque transformation métier est encapsulée dans une fonction dédiée, testable indépendamment. Les fonctions complexes sont décomposées en sous-fonctions pour améliorer la lisibilité.

**Documentation exhaustive** : chaque fonction critique comporte des docstrings détaillées expliquant la logique métier, les paramètres attendus, et les références au code SAS équivalent. Cette documentation facilite la reprise du code par d'autres développeurs.

**Logging structuré** : des logs détaillés tracent chaque étape de transformation (nombre de lignes lues, nombre de lignes filtrées, résultats des jointures) pour faciliter le débogage.

### Défis techniques rencontrés

Plusieurs défis ont nécessité des solutions spécifiques :

**Extraction de capitaux depuis 14 colonnes génériques** : le code SAS cherchait des mots-clés (SMP, LCI, PE, RD) dans 14 paires de colonnes MTCAPI/LBCAPI. Une logique itérative qui parcourt ces colonnes dans l'ordre et s'arrête dès qu'un capital est trouvé a été implémentée, reproduisant fidèlement la logique SAS.

**Gestion de la migration IMS/AZEC** : les polices migrées d'AZEC vers IMS ne doivent apparaître que dans AZ (pour éviter la duplication). Un filtre basé sur une table de référence listant les polices migrées par date a été implémenté, appliqué en fonction de la vision traitée.

**Enrichissement NAF par FULL OUTER JOIN** : le code SAS réalisait un FULL OUTER JOIN sur quatre tables pour obtenir le code NAF. Cette logique a été reproduite en PySpark avec `df1.join(df2, how='full_outer').join(df3, how='full_outer')...` puis application de `coalesce()` pour sélectionner le premier code NAF non-null.

---

## 3.5 Phase 5 : Validation et tests (5 semaines)

La validation constituait l'enjeu critique du projet : démontrer que les résultats Python sont strictement identiques aux résultats SAS. Une stratégie de validation en trois dimensions a été appliquée.

**[FIGURE 3.3 : Stratégie de validation en 3 dimensions - Structurel, Fonctionnel, Performance]**

### Validation structurelle

Cette dimension vérifiait la cohérence technique des données produites, indépendamment des valeurs métier.

**Audit des 45 datasets sources** : vérification que tous les fichiers sources nécessaires étaient bien accessibles et lisibles, avec les bons schémas de données.

**Vérification des types de données** : contrôle que chaque colonne produite avait le bon type (string, integer, decimal, date) et le bon format.

**Validation des schémas** : comparaison des schémas Python et SAS pour s'assurer qu'aucune colonne n'était manquante ou en trop.

### Validation fonctionnelle

Cette dimension garantissait la parité métier entre Python et SAS. 

**Comparaison des KPI sur 5 visions mensuelles** : pour chaque vision (mois), les indicateurs clés (nombre de polices PTF/AFN/RES, somme des capitaux, total des primes) ont été calculés côté Python et côté SAS, puis les résultats ont été comparés. La tolérance acceptée était **< 0,01%** d'écart.

Les tests ont été réalisés sur cinq visions différentes (de juin 2023 à octobre 2023) pour valider la stabilité des résultats dans le temps et détecter d'éventuels problèmes liés à des données spécifiques de certains mois.

**Analyse des écarts** : chaque écart détecté (même minime) a été investigué jusqu'à identifier sa cause racine. Dans la plupart des cas, les écarts provenaient de différences de gestion des arrondis entre SAS et Python, ou de logiques de tri différentes dans les jointures. Des ajustements ont été apportés pour garantir une parité stricte.

### Validation de performance

Cette dimension mesurait les gains de performance apportés par PySpark :

**Benchmark des temps d'exécution** : mesure des durées de traitement pour chaque pipeline (PTF_MVT, Capitaux, Émissions) en Python vs SAS. PySpark a montré des gains de 30% à 50% selon les pipelines grâce au calcul distribué.

**Optimisation de la consommation mémoire** : monitoring de l'usage mémoire Spark et optimisation des configurations (partitionnement, broadcast joins) pour éviter les dépassements mémoire sur les gros volumes.

### Résultats de validation

Les tests de validation ont démontré la **parité fonctionnelle stricte** entre Python et SAS :
- ✅ **100% des datasets sources** correctement ingérés
- ✅ **Écarts < 0,01%** sur tous les KPI pour les 5 visions testées
- ✅ **Gains de performance de 30-50%** selon les pipelines
- ✅ **Architecture moderne et maintenable** prête pour les futures évolutions

Ces résultats ont validé la faisabilité technique de la migration et ouvert la voie à la migration des autres marchés P&C.
